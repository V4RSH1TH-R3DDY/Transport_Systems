{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“Š TRAF-GNN Data Exploration\n",
                "\n",
                "This notebook explores the METR-LA traffic dataset:\n",
                "- Load and inspect raw data\n",
                "- Analyze data quality and missing values\n",
                "- Visualize temporal traffic patterns\n",
                "- Explore spatial relationships"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import h5py\n",
                "import pickle\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set plotting style\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette('husl')\n",
                "%matplotlib inline\n",
                "\n",
                "print(\"âœ… Imports successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data paths\n",
                "data_dir = Path('../data/raw')\n",
                "h5_file = data_dir / 'metr-la.h5'\n",
                "adj_file = data_dir / 'adj_mx.pkl'\n",
                "sensor_ids_file = data_dir / 'graph_sensor_ids.txt'\n",
                "sensor_locs_file = data_dir / 'graph_sensor_locations.csv'\n",
                "\n",
                "# Check if files exist\n",
                "print(\"Checking data files...\")\n",
                "print(f\"H5 file: {h5_file.exists()}\")\n",
                "print(f\"Adjacency matrix: {adj_file.exists()}\")\n",
                "print(f\"Sensor IDs: {sensor_ids_file.exists()}\")\n",
                "print(f\"Sensor locations: {sensor_locs_file.exists()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load traffic speed data\n",
                "with h5py.File(h5_file, 'r') as f:\n",
                "    # Print available keys\n",
                "    print(\"Available keys in H5 file:\")\n",
                "    print(list(f.keys()))\n",
                "    \n",
                "    # Load data (usually stored under 'speed' or 'data' key)\n",
                "    # Adjust key name if different\n",
                "    data = f['speed'][:] if 'speed' in f.keys() else f[list(f.keys())[0]][:]\n",
                "\n",
                "print(f\"\\nðŸ“Š Data shape: {data.shape}\")\n",
                "print(f\"   Timesteps: {data.shape[0]:,}\")\n",
                "print(f\"   Sensors: {data.shape[1]}\")\n",
                "print(f\"   Duration: ~{data.shape[0] * 5 / 60 / 24:.1f} days\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load adjacency matrix\n",
                "with open(adj_file, 'rb') as f:\n",
                "    sensor_ids, sensor_id_to_ind, adj_mx = pickle.load(f, encoding='latin1')\n",
                "\n",
                "print(f\"\\nðŸ—ºï¸ Adjacency matrix shape: {adj_mx.shape}\")\n",
                "print(f\"   Number of sensors: {len(sensor_ids)}\")\n",
                "print(f\"   Number of edges: {np.sum(adj_mx > 0)}\")\n",
                "print(f\"   Graph density: {np.sum(adj_mx > 0) / (adj_mx.shape[0] * adj_mx.shape[1]):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load sensor locations (if available)\n",
                "try:\n",
                "    sensor_locs = pd.read_csv(sensor_locs_file)\n",
                "    print(\"\\nðŸ“ Sensor locations:\")\n",
                "    print(sensor_locs.head())\n",
                "except:\n",
                "    print(\"\\nâš ï¸  Sensor locations file not found or has different format\")\n",
                "    sensor_locs = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Quality Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "missing_mask = np.isnan(data)\n",
                "missing_pct = np.mean(missing_mask) * 100\n",
                "\n",
                "print(f\"\\nðŸ“‰ Missing Values Analysis:\")\n",
                "print(f\"   Total missing: {missing_pct:.2f}%\")\n",
                "print(f\"   Missing entries: {np.sum(missing_mask):,}\")\n",
                "\n",
                "# Missing values per sensor\n",
                "missing_per_sensor = np.mean(missing_mask, axis=0) * 100\n",
                "print(f\"\\n   Sensors with >10% missing: {np.sum(missing_per_sensor > 10)}\")\n",
                "print(f\"   Max missing in a sensor: {np.max(missing_per_sensor):.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize missing data pattern\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Missing values per sensor\n",
                "axes[0].bar(range(len(missing_per_sensor)), sorted(missing_per_sensor, reverse=True))\n",
                "axes[0].set_xlabel('Sensor (sorted by missingness)')\n",
                "axes[0].set_ylabel('Missing %')\n",
                "axes[0].set_title('Missing Values per Sensor')\n",
                "axes[0].axhline(y=10, color='r', linestyle='--', label='10% threshold')\n",
                "axes[0].legend()\n",
                "\n",
                "# Missing values over time\n",
                "missing_per_time = np.mean(missing_mask, axis=1) * 100\n",
                "axes[1].plot(missing_per_time, alpha=0.7)\n",
                "axes[1].set_xlabel('Timestep')\n",
                "axes[1].set_ylabel('Missing %')\n",
                "axes[1].set_title('Missing Values Over Time')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Traffic speed statistics\n",
                "valid_data = data[~missing_mask]\n",
                "\n",
                "print(f\"\\nðŸš— Traffic Speed Statistics (mph):\")\n",
                "print(f\"   Mean: {np.mean(valid_data):.2f}\")\n",
                "print(f\"   Median: {np.median(valid_data):.2f}\")\n",
                "print(f\"   Std Dev: {np.std(valid_data):.2f}\")\n",
                "print(f\"   Min: {np.min(valid_data):.2f}\")\n",
                "print(f\"   Max: {np.max(valid_data):.2f}\")\n",
                "\n",
                "# Speed distribution\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.hist(valid_data, bins=50, edgecolor='black', alpha=0.7)\n",
                "plt.xlabel('Speed (mph)')\n",
                "plt.ylabel('Frequency')\n",
                "plt.title('Distribution of Traffic Speeds')\n",
                "plt.axvline(np.mean(valid_data), color='r', linestyle='--', label=f'Mean: {np.mean(valid_data):.1f}')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Temporal Pattern Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select a few sensors for visualization\n",
                "sample_sensors = [0, 50, 100, 150, 200]  # Sample 5 sensors\n",
                "\n",
                "# Plot traffic speed over time for sample sensors\n",
                "plt.figure(figsize=(15, 6))\n",
                "for sensor in sample_sensors:\n",
                "    plt.plot(data[:1000, sensor], alpha=0.7, label=f'Sensor {sensor}')\n",
                "\n",
                "plt.xlabel('Timestep (5-min intervals)')\n",
                "plt.ylabel('Speed (mph)')\n",
                "plt.title('Traffic Speed Time Series (First 1000 timesteps)')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Daily pattern (assuming 5-min intervals: 288 intervals per day)\n",
                "intervals_per_day = 288\n",
                "\n",
                "# Average speed by time of day across all sensors and days\n",
                "n_days = data.shape[0] // intervals_per_day\n",
                "daily_data = data[:n_days * intervals_per_day].reshape(n_days, intervals_per_day, -1)\n",
                "avg_daily_pattern = np.nanmean(daily_data, axis=(0, 2))\n",
                "\n",
                "# Time labels (hours)\n",
                "hours = np.arange(24)\n",
                "hourly_speed = [np.nanmean(avg_daily_pattern[i*12:(i+1)*12]) for i in range(24)]\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "plt.plot(hours, hourly_speed, marker='o', linewidth=2)\n",
                "plt.xlabel('Hour of Day')\n",
                "plt.ylabel('Average Speed (mph)')\n",
                "plt.title('Average Traffic Speed by Hour of Day')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.xticks(range(0, 24, 2))\n",
                "plt.axvspan(7, 9, alpha=0.2, color='red', label='Morning Rush')\n",
                "plt.axvspan(17, 19, alpha=0.2, color='orange', label='Evening Rush')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nâ° Peak Traffic Insights:\")\n",
                "print(f\"   Slowest hour: {hours[np.argmin(hourly_speed)]}:00 ({min(hourly_speed):.1f} mph)\")\n",
                "print(f\"   Fastest hour: {hours[np.argmax(hourly_speed)]}:00 ({max(hourly_speed):.1f} mph)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Spatial Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize adjacency matrix\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.imshow(adj_mx, cmap='viridis', aspect='auto')\n",
                "plt.colorbar(label='Connection Weight')\n",
                "plt.xlabel('Sensor ID')\n",
                "plt.ylabel('Sensor ID')\n",
                "plt.title('Physical Road Network Adjacency Matrix')\n",
                "plt.show()\n",
                "\n",
                "# Degree distribution\n",
                "degrees = np.sum(adj_mx > 0, axis=1)\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.hist(degrees, bins=20, edgecolor='black', alpha=0.7)\n",
                "plt.xlabel('Node Degree')\n",
                "plt.ylabel('Frequency')\n",
                "plt.title('Degree Distribution of Road Network')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nðŸ“Š Network Statistics:\")\n",
                "print(f\"   Average degree: {np.mean(degrees):.2f}\")\n",
                "print(f\"   Max degree: {np.max(degrees)}\")\n",
                "print(f\"   Min degree: {np.min(degrees)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sensor map (if location data available)\n",
                "if sensor_locs is not None and 'latitude' in sensor_locs.columns:\n",
                "    plt.figure(figsize=(12, 8))\n",
                "    plt.scatter(sensor_locs['longitude'], sensor_locs['latitude'], \n",
                "                c=np.nanmean(data, axis=0), cmap='RdYlGn', s=50, alpha=0.6)\n",
                "    plt.colorbar(label='Average Speed (mph)')\n",
                "    plt.xlabel('Longitude')\n",
                "    plt.ylabel('Latitude')\n",
                "    plt.title('Sensor Locations Colored by Average Speed')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"âš ï¸  Sensor location data not available for mapping\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Correlation Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate correlation between sensors (sample subset for speed)\n",
                "sample_size = 50  # Sample first 50 sensors\n",
                "subset_data = data[:, :sample_size]\n",
                "\n",
                "# Fill NaN values with column mean for correlation calculation\n",
                "subset_filled = np.copy(subset_data)\n",
                "for i in range(subset_filled.shape[1]):\n",
                "    col_mean = np.nanmean(subset_filled[:, i])\n",
                "    subset_filled[np.isnan(subset_filled[:, i]), i] = col_mean\n",
                "\n",
                "corr_matrix = np.corrcoef(subset_filled.T)\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(corr_matrix, cmap='coolwarm', center=0, \n",
                "            vmin=-1, vmax=1, square=True)\n",
                "plt.title(f'Correlation Matrix (First {sample_size} Sensors)')\n",
                "plt.xlabel('Sensor ID')\n",
                "plt.ylabel('Sensor ID')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nðŸ”— Correlation Insights:\")\n",
                "print(f\"   Average correlation: {np.mean(corr_matrix[np.triu_indices_from(corr_matrix, k=1)]):.3f}\")\n",
                "print(f\"   Max correlation: {np.max(corr_matrix[np.triu_indices_from(corr_matrix, k=1)]):.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary & Next Steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"ðŸ“ DATA EXPLORATION SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nâœ… Dataset: METR-LA\")\n",
                "print(f\"   - Sensors: {data.shape[1]}\")\n",
                "print(f\"   - Timesteps: {data.shape[0]:,} (5-min intervals)\")\n",
                "print(f\"   - Duration: ~{data.shape[0] * 5 / 60 / 24:.1f} days\")\n",
                "print(f\"   - Missing data: {missing_pct:.2f}%\")\n",
                "print(f\"\\nâœ… Traffic Characteristics:\")\n",
                "print(f\"   - Average speed: {np.mean(valid_data):.1f} mph\")\n",
                "print(f\"   - Speed range: {np.min(valid_data):.1f} - {np.max(valid_data):.1f} mph\")\n",
                "print(f\"\\nâœ… Network Structure:\")\n",
                "print(f\"   - Nodes: {adj_mx.shape[0]}\")\n",
                "print(f\"   - Edges: {np.sum(adj_mx > 0)}\")\n",
                "print(f\"   - Avg degree: {np.mean(degrees):.1f}\")\n",
                "print(f\"\\nðŸ“‹ Next Steps:\")\n",
                "print(\"   1. Handle missing values (interpolation/imputation)\")\n",
                "print(\"   2. Normalize traffic speeds\")\n",
                "print(\"   3. Create train/validation/test splits\")\n",
                "print(\"   4. Build multi-view graphs\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}